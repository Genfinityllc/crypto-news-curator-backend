#!/usr/bin/env python3
"""
SDXL Universal LoRA Cover Generator - REAL VERSION
Uses SDXL pipeline with Universal LoRA for high-quality crypto news covers
"""
import os
import random
import base64
import io
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import uvicorn
import logging
from PIL import Image, ImageDraw, ImageFont
import torch
from diffusers import StableDiffusionXLPipeline
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Log system information
logger.info("üñ•Ô∏è System Information:")
logger.info(f"üìä Python version: {__import__('sys').version}")
logger.info(f"üîß Platform: {__import__('platform').platform()}")

try:
    import torch
    logger.info(f"üî• PyTorch version: {torch.__version__}")
    logger.info(f"‚ö° CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"üéÆ GPU count: {torch.cuda.device_count()}")
        logger.info(f"üéØ Current GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
except ImportError:
    logger.warning("‚ö†Ô∏è PyTorch not available")

try:
    import diffusers
    logger.info(f"üåÄ Diffusers version: {diffusers.__version__}")
except ImportError:
    logger.warning("‚ö†Ô∏è Diffusers not available")

app = FastAPI(
    title="SDXL Universal LoRA Cover Generator",
    description="Generate crypto news covers using SDXL + Universal LoRA",
    version="6.0.0"
)

class GenerationRequest(BaseModel):
    title: str
    subtitle: str = "CRYPTO NEWS"
    client: str = "hedera"
    style: Optional[str] = "dark_theme"
    use_trained_lora: bool = True  # Default to True for Universal LoRA

class GenerationResponse(BaseModel):
    success: bool
    image_url: Optional[str] = None
    error: Optional[str] = None
    metadata: Optional[dict] = None

class SDXLLoRACoverGenerator:
    def __init__(self):
        self.watermark = None
        self.pipeline = None
        self.current_lora = None
        self.lora_available = {}
        
        # Initialize
        self.load_watermark()
        self.check_available_loras()
        
        # Try to load pipeline with detailed error reporting
        try:
            logger.info("üöÄ Initializing SDXL pipeline...")
            self.load_pipeline()
            logger.info("‚úÖ SDXL pipeline initialization completed")
        except ImportError as e:
            logger.error(f"‚ùå Missing dependencies for SDXL: {e}")
            logger.info("üìù Install missing packages or falling back to programmatic generation")
            self.pipeline = None
        except RuntimeError as e:
            logger.error(f"‚ùå SDXL runtime error (likely memory): {e}")
            logger.info("üìù Try GPU hardware or falling back to programmatic generation")
            self.pipeline = None
        except Exception as e:
            logger.error(f"‚ùå Unexpected SDXL error: {e}")
            logger.error(f"üìä Error details: {type(e).__name__}: {str(e)}")
            logger.info("üìù Falling back to programmatic generation")
            self.pipeline = None
    
    def load_watermark(self):
        """Load watermark if available"""
        watermark_path = "genfinity-watermark.png"
        try:
            if os.path.exists(watermark_path):
                self.watermark = Image.open(watermark_path).convert("RGBA")
                logger.info(f"‚úÖ Loaded watermark: {self.watermark.size}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Watermark loading failed: {e}")
    
    def check_available_loras(self):
        """Check which LoRA models are available"""
        lora_dir = "models/lora"
        if not os.path.exists(lora_dir):
            logger.warning(f"‚ö†Ô∏è LoRA directory not found: {lora_dir}")
            return
        
        # Check for style-specific LoRAs
        styles = ["energy_fields", "dark_theme", "network_nodes", "particle_waves"]
        clients = ["hedera", "algorand", "constellation"]
        
        for client in clients:
            for style in styles:
                lora_path = f"{lora_dir}/{client}_{style}_lora.safetensors"
                if os.path.exists(lora_path):
                    self.lora_available[f"{client}_{style}"] = lora_path
                    logger.info(f"‚úÖ Found LoRA: {client}_{style}")
        
        # Check for universal style LoRA
        universal_lora = f"{lora_dir}/crypto_cover_styles_lora.safetensors"
        if os.path.exists(universal_lora):
            self.lora_available["universal"] = universal_lora
            logger.info("‚úÖ Found LoRA: universal")
        
        if not self.lora_available:
            logger.info("üìù No trained LoRA models found - will use programmatic fallback")
    
    def load_pipeline(self):
        """Load SDXL pipeline for LoRA generation with aggressive optimizations"""
        import gc
        
        # Force garbage collection before loading
        gc.collect()
        
        if torch.cuda.is_available():
            device = "cuda"
            torch_dtype = torch.float16
            logger.info(f"üöÄ CUDA available - GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            # Clear CUDA cache
            torch.cuda.empty_cache()
        else:
            device = "cpu"
            torch_dtype = torch.float32
            logger.info("üöÄ Using CPU for SDXL")
        
        logger.info(f"üöÄ Loading SDXL pipeline on {device} with {torch_dtype}")
        
        try:
            # Load with maximum memory optimizations
            self.pipeline = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch_dtype,
                variant="fp16" if device == "cuda" and torch_dtype == torch.float16 else None,
                use_safetensors=True,
                low_cpu_mem_usage=True,
                device_map="auto" if device == "cuda" else None,
                max_memory={0: "15GB"} if device == "cuda" else None
            )
            
            logger.info("üì¶ SDXL model loaded, moving to device...")
            
            # Move to device with error handling
            if device == "cuda":
                self.pipeline = self.pipeline.to(device)
                
                # Enable all GPU optimizations
                try:
                    self.pipeline.enable_xformers_memory_efficient_attention()
                    logger.info("‚úÖ XFormers enabled")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è XFormers failed: {e}")
                
                try:
                    self.pipeline.enable_model_cpu_offload()
                    logger.info("‚úÖ Model CPU offload enabled")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è CPU offload failed: {e}")
            else:
                # CPU optimizations
                try:
                    self.pipeline.enable_sequential_cpu_offload()
                    logger.info("‚úÖ Sequential CPU offload enabled")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Sequential CPU offload failed: {e}")
            
            # Test pipeline with a simple generation
            logger.info("üß™ Testing pipeline with simple generation...")
            test_result = self.pipeline(
                "test",
                height=64,
                width=64,
                num_inference_steps=1,
                guidance_scale=1.0
            )
            logger.info("‚úÖ SDXL Pipeline loaded and tested successfully")
            
        except Exception as e:
            logger.error(f"‚ùå SDXL Pipeline loading failed: {str(e)}")
            logger.error(f"üìä Error type: {type(e).__name__}")
            self.pipeline = None
            raise e
    
    def load_lora_weights(self, client, style):
        """Load appropriate LoRA weights"""
        if not self.pipeline:
            return False
        
        # Try specific client/style LoRA first
        lora_key = f"{client}_{style}"
        if lora_key in self.lora_available:
            lora_path = self.lora_available[lora_key]
        elif "universal" in self.lora_available:
            lora_path = self.lora_available["universal"]
            lora_key = "universal"
        else:
            logger.info(f"üìù No LoRA available for {client}/{style}")
            return False
        
        try:
            # Unload previous LoRA
            if self.current_lora:
                self.pipeline.unload_lora_weights()
            
            # Load new LoRA
            self.pipeline.load_lora_weights(lora_path)
            self.current_lora = lora_key
            logger.info(f"‚úÖ Loaded LoRA: {lora_key}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load LoRA {lora_path}: {e}")
            return False
    
    def generate_with_trained_lora(self, client, style, title):
        """Generate background using trained LoRA model"""
        if not self.pipeline:
            return None
        
        # Load appropriate LoRA
        lora_loaded = self.load_lora_weights(client, style)
        
        # Create style-specific prompt based on original training data
        style_prompts = {
            "energy_fields": f"crypto news cover background, glowing energy fields, particle effects, cosmic energy, vibrant auras, {client} branding colors, professional design",
            "dark_theme": f"crypto news cover background, dark professional background, subtle geometric patterns, minimal lighting, {client} color scheme, corporate style",
            "network_nodes": f"crypto news cover background, connected network nodes, digital connections, tech visualization, {client} branding, futuristic design",
            "particle_waves": f"crypto news cover background, flowing particle waves, dynamic motion, wave patterns, {client} colors, energy flow"
        }
        
        prompt = style_prompts.get(style, f"crypto news cover background, {client} style, professional design")
        negative_prompt = "text, letters, words, title, subtitle, watermark, signature, blurry, low quality, distorted, people, faces"
        
        logger.info(f"üé® Generating LoRA background: {client}/{style}")
        
        try:
            # Generate background image
            image = self.pipeline(
                prompt=prompt,
                negative_prompt=negative_prompt,
                height=512,
                width=1024,  # 2:1 aspect ratio like your covers
                num_inference_steps=30,
                guidance_scale=7.5,
                num_images_per_prompt=1,
                generator=torch.Generator().manual_seed(random.randint(1, 1000000))
            ).images[0]
            
            # Upscale to final resolution
            image = image.resize((1800, 900), Image.Resampling.LANCZOS)
            
            logger.info("‚úÖ LoRA background generated successfully")
            return image
            
        except Exception as e:
            logger.error(f"‚ùå LoRA generation failed: {e}")
            return None
    
    def generate_programmatic_fallback(self, width, height, client, style):
        """Fallback to programmatic generation if LoRA unavailable"""
        logger.info(f"üìù Using programmatic fallback for {client}/{style}")
        
        # Client colors (same as before)
        colors = {
            'hedera': {
                'primary': (138, 43, 226),
                'secondary': (75, 0, 130),
                'accent': (186, 85, 211),
                'energy': (255, 0, 255)
            },
            'algorand': {
                'primary': (0, 120, 140),
                'secondary': (0, 85, 100),
                'accent': (75, 163, 224),
                'energy': (0, 255, 255)
            },
            'constellation': {
                'primary': (72, 61, 139),
                'secondary': (25, 25, 112),
                'accent': (106, 90, 205),
                'energy': (255, 255, 255)
            }
        }
        
        client_colors = colors.get(client, colors['hedera'])
        img = Image.new('RGB', (width, height), (10, 10, 15))
        draw = ImageDraw.Draw(img)
        
        # Create enhanced visual elements (same as ultra-visible version)
        if style == "dark_theme":
            # Dark gradient
            for y in range(height):
                darkness = 0.8 + 0.2 * (y / height)
                color = tuple(int(c * darkness * 0.15) for c in client_colors['primary'])
                draw.line([(0, y), (width, y)], fill=color)
            
            # Geometric patterns
            pattern_img = Image.new('RGBA', (width, height), (0, 0, 0, 0))
            pattern_draw = ImageDraw.Draw(pattern_img)
            
            hex_size = 80
            for y in range(0, height + hex_size, hex_size):
                for x in range(0, width + hex_size, hex_size):
                    offset_x = x + (hex_size // 2 if (y // hex_size) % 2 else 0)
                    alpha = random.randint(40, 100)
                    hex_color = client_colors['accent'] + (alpha,)
                    
                    points = [
                        (offset_x, y - hex_size//3),
                        (offset_x + hex_size//3, y),
                        (offset_x, y + hex_size//3),
                        (offset_x - hex_size//3, y)
                    ]
                    pattern_draw.polygon(points, outline=hex_color, width=2)
            
            img = Image.alpha_composite(img.convert('RGBA'), pattern_img).convert('RGB')
        
        return img
    
    def get_fonts(self):
        """Get fonts for text overlay"""
        fonts = {}
        font_sizes = {"title": 200, "subtitle": 100, "small": 60}
        
        font_paths = [
            "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
        ]
        
        for size_name, size in font_sizes.items():
            fonts[size_name] = None
            for font_path in font_paths:
                try:
                    if os.path.exists(font_path):
                        fonts[size_name] = ImageFont.truetype(font_path, size)
                        break
                except:
                    continue
            
            if fonts[size_name] is None:
                fonts[size_name] = ImageFont.load_default()
        
        return fonts
    
    def create_text_overlay(self, width, height, title, subtitle, fonts):
        """Create text overlay"""
        overlay = Image.new("RGBA", (width, height), (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)
        
        title_y = height // 4
        
        if title:
            title = title.upper()
            words = title.split()
            if len(words) > 3:
                mid = len(words) // 2
                title_lines = [" ".join(words[:mid]), " ".join(words[mid:])]
            else:
                title_lines = [title]
            
            for i, line in enumerate(title_lines):
                bbox = draw.textbbox((0, 0), line, font=fonts["title"])
                text_width = bbox[2] - bbox[0]
                x = (width - text_width) // 2
                y = title_y + (i * 250)
                
                # Strong shadows
                draw.text((x + 8, y + 8), line, fill=(0, 0, 0, 255), font=fonts["title"])
                draw.text((x + 4, y + 4), line, fill=(0, 0, 0, 200), font=fonts["title"])
                # White text
                draw.text((x, y), line, fill=(255, 255, 255, 255), font=fonts["title"])
        
        # Subtitle
        if subtitle:
            subtitle_y = title_y + len(title_lines) * 250 + 80
            bbox = draw.textbbox((0, 0), subtitle, font=fonts["subtitle"])
            text_width = bbox[2] - bbox[0]
            x = (width - text_width) // 2
            
            # Box background
            padding = 40
            draw.rounded_rectangle([x-padding, subtitle_y-20, x+text_width+padding, subtitle_y+120], 
                                 radius=20, fill=(0, 0, 0, 180))
            
            # Subtitle text
            draw.text((x + 3, subtitle_y + 3), subtitle, fill=(0, 0, 0, 200), font=fonts["subtitle"])
            draw.text((x, subtitle_y), subtitle, fill=(255, 255, 255, 255), font=fonts["subtitle"])
        
        return overlay
    
    def generate_cover(self, title, subtitle, client="hedera", style="dark_theme", use_trained_lora=True):
        """Generate complete cover with background and text"""
        try:
            logger.info(f"üé® Generating cover: {title} (client: {client}, style: {style}, LoRA: {use_trained_lora})")
            
            width, height = 1800, 900
            
            # Try LoRA generation first if enabled and available
            background = None
            if use_trained_lora and self.lora_available and self.pipeline:
                background = self.generate_with_trained_lora(client, style, title)
            
            # Fallback to programmatic if LoRA failed or disabled
            if background is None:
                background = self.generate_programmatic_fallback(width, height, client, style)
            
            # Convert to RGBA for compositing
            base_rgba = background.convert("RGBA")
            
            # Add text overlay
            fonts = self.get_fonts()
            text_overlay = self.create_text_overlay(width, height, title, subtitle, fonts)
            base_rgba = Image.alpha_composite(base_rgba, text_overlay)
            
            # Add watermark
            if self.watermark:
                watermark_resized = self.watermark.resize((width, height), Image.Resampling.LANCZOS)
                final_image = Image.alpha_composite(base_rgba, watermark_resized)
            else:
                final_image = base_rgba
            
            generation_method = "sdxl_universal_lora" if (use_trained_lora and self.current_lora) else "programmatic_fallback"
            logger.info(f"‚úÖ Cover generated using: {generation_method}")
            
            return final_image.convert("RGB"), generation_method
            
        except Exception as e:
            logger.error(f"‚ùå Cover generation failed: {str(e)}")
            return None, "error"

# Initialize generator
generator = SDXLLoRACoverGenerator()

@app.get("/")
async def root():
    """Root endpoint"""
    available_loras = list(generator.lora_available.keys()) if generator.lora_available else ["none"]
    
    return {
        "message": "SDXL Universal LoRA Cover Generator",
        "status": "running",
        "available_loras": available_loras,
        "pipeline_loaded": generator.pipeline is not None,
        "endpoints": ["/generate", "/health", "/lora-status"],
        "version": "6.0.0"
    }

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy", 
        "service": "sdxl-universal-lora-cover-generator",
        "lora_count": len(generator.lora_available)
    }

@app.get("/lora-status")
async def lora_status():
    """Get LoRA model status"""
    return {
        "available_loras": generator.lora_available,
        "current_lora": generator.current_lora,
        "pipeline_loaded": generator.pipeline is not None
    }

@app.post("/generate")
async def generate_image(request: GenerationRequest):
    """Generate cover using SDXL Universal LoRA or fallback"""
    try:
        logger.info(f"üé® Request: {request.title} (LoRA: {request.use_trained_lora})")
        
        # Generate the cover
        result = generator.generate_cover(
            title=request.title,
            subtitle=request.subtitle,
            client=request.client,
            style=request.style or "dark_theme",
            use_trained_lora=request.use_trained_lora
        )
        
        if result[0] is None:
            raise HTTPException(status_code=500, detail="Generation failed")
        
        image, method = result
        
        # Convert to base64
        buffer = io.BytesIO()
        image.save(buffer, format="PNG", quality=95)
        buffer.seek(0)
        image_data = buffer.getvalue()
        base64_image = base64.b64encode(image_data).decode()
        
        return GenerationResponse(
            success=True,
            image_url=f"data:image/png;base64,{base64_image}",
            metadata={
                "client": request.client,
                "style": request.style,
                "title": request.title,
                "subtitle": request.subtitle,
                "generation_method": method,
                "lora_used": generator.current_lora,
                "resolution": "1800x900"
            }
        )
        
    except Exception as e:
        logger.error(f"‚ùå Generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 7860))
    logger.info(f"üöÄ Starting SDXL Universal LoRA Cover Generator on port {port}")
    uvicorn.run(app, host="0.0.0.0", port=port)